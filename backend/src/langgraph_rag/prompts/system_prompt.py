from typing import List
from ..llm.bedrock_llm import BedrockLLM
from ..utils.logger_utils import get_logger
from ..utils.config_utils import BaseConfig
from ..utils.llm_utils import TextChatMessage
from .one_shot import TTHC_ONESHOT


logger = get_logger(__name__)


class GenerateAnswer:
        def __init__(self, global_config : BaseConfig):
                self.bedrock_llm = BedrockLLM(global_config= global_config)

        def generate_intent(self, question: str, related_text: str, conversation_history: List[TextChatMessage]):


                prompt: List[TextChatMessage] = [
                        {
                        "role": "system", 
                        "content": f"""\
                        B·∫°n l√† m·ªôt chuy√™n gia ph√°p l√Ω, c√≥ nhi·ªám v·ª• h·ªó tr·ª£ ng∆∞·ªùi d√¢n v√† c√°n b·ªô trong vi·ªác t√¨m hi·ªÉu v√† h∆∞·ªõng d·∫´n c√°c **th·ªß t·ª•c h√†nh ch√≠nh t·∫°i Vi·ªát Nam**.
                        B·∫°n s·∫Ω ƒë∆∞·ª£c cung c·∫•p:

                        M·ªôt c√¢u h·ªèi do ng∆∞·ªùi d√πng ƒë·∫∑t ra (User Question).

                        M·ªôt ƒëo·∫°n t√†i li·ªáu tr√≠ch xu·∫•t t·ª´ h·ªá th·ªëng vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t ho·∫∑c c·ªïng d·ªãch v·ª• c√¥ng (Context Documents).

                        H√£y ƒë·ªçc k·ªπ ƒëo·∫°n tr√≠ch xu·∫•t v√† **ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n n·ªôi dung t√†i li·ªáu** ƒë∆∞·ª£c cung c·∫•p. N·∫øu kh√¥ng c√≥ ƒë·ªß th√¥ng tin, h√£y tr·∫£ l·ªùi trung th·ª±c l√† **kh√¥ng t√¨m th·∫•y** ho·∫∑c **kh√¥ng r√µ**.
                        H√£y tr·∫£ l·ªùi **r√µ r√†ng, chi ti·∫øt, ch√≠nh x√°c, ƒë√∫ng n·ªôi dung vƒÉn b·∫£n** v√† ƒë·∫ßy ƒë·ªß cƒÉn c·ª© n·∫øu c·∫ßn thi·∫øt.

                        T√†i li·ªáu tr√≠ch xu·∫•t:
                        {related_text}

                        D·ª±a tr√™n n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, h√£y tr·∫£ l·ªùi c√¢u h·ªèi tr√™n.  
                        N·∫øu th√¥ng tin trong t√†i li·ªáu kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ l√† "Kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu".
                        
                     
                        """
                        },
                        {
                        "role": "user", 
                        "content": f"""
                                Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch
                                C√¢u h·ªèi ng∆∞·ªùi d√πng:
                                \"\"\"{question}\"\"\"
                                Tr·∫£ l·ªùi:
                        """
                        },
                ]

                        #V√≠ d·ª•: 

                        #{TTHC_ONESHOT}
                        #L∆ØU √ù: khi tr·∫£ l·ªùi ph·∫£i c√≥ ngu·ªìn tr√≠ch d·∫´n v√† gi·∫•y t·ªù ƒë√≠nh k√®m n·∫øu c√≥

                messages: List[TextChatMessage] = conversation_history + prompt

                response_text, metadata, cached = self.bedrock_llm.infer(messages=messages)
                total_token = metadata['prompt_tokens'] + metadata['completion_tokens']
                return response_text, total_token


        def generate_general(self, question: str,  conversation_history: List[TextChatMessage]):
                prompt: List[TextChatMessage] = [
                        {
                        "role": "system", 
                        "content": f"""\
                        B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, gi√†u ki·∫øn th·ª©c v√† lu√¥n tr·∫£ l·ªùi r√µ r√†ng, ch√≠nh x√°c.

                        Nguy√™n t·∫Øc khi tr·∫£ l·ªùi:
                        1. Hi·ªÉu k·ªπ c√¢u h·ªèi, n·∫øu c√¢u h·ªèi m∆° h·ªì th√¨ gi·∫£ ƒë·ªãnh h·ª£p l√Ω ƒë·ªÉ tr·∫£ l·ªùi.
                        2. Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi theo c·∫•u tr√∫c:
                        - Tr·∫£ l·ªùi ng·∫Øn g·ªçn, tr·ª±c ti·∫øp v√†o tr·ªçng t√¢m.
                        - N·∫øu c·∫ßn, gi·∫£i th√≠ch chi ti·∫øt v√† ƒë∆∞a v√≠ d·ª• minh h·ªça.
                        3. Lu√¥n s·ª≠ d·ª•ng ti·∫øng Vi·ªát r√µ r√†ng, d·ªÖ hi·ªÉu. Tr√°nh t·ª´ ng·ªØ m∆° h·ªì.
                        Y√™u c·∫ßu ƒë·ªãnh d·∫°ng ƒë·∫ßu ra:
                        - Ch·ªâ tr·∫£ l·ªùi, kh√¥ng l·∫∑p l·∫°i c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
                        - Gi·ªØ c√¢u tr·∫£ l·ªùi g·ªçn g√†ng nh∆∞ng ƒë·∫ßy ƒë·ªß th√¥ng tin c·∫ßn thi·∫øt.
                        """
                        },
                        {
                        "role": "user", 
                        "content": f"""
                                Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi C√¢u h·ªèi ng∆∞·ªùi d√πng:
                                \"\"\"{question}\"\"\"
                                Tr·∫£ l·ªùi:
                        """
                        },
                ]
                messages: List[TextChatMessage] = conversation_history + prompt
                response_text, metadata, cached = self.bedrock_llm.infer(messages=messages)
                total_token = metadata['prompt_tokens'] + metadata['completion_tokens']
                return response_text, total_token


# # run: python -m langgraph_rag.prompts.system_prompt
# if __name__ == "__main__":
#     g_a = GenerateAnswer(global_config=BaseConfig())

#     response = g_a.generate_general(question="Ch·ªó n√†o kh√¥ng ƒë∆∞·ª£c ph√©p ƒëƒÉng k√Ω t·∫°m tr√∫ m·ªõi?")

#     print("üì® Response:", response)


